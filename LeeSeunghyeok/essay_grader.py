# essay_grader.py

# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---

# í™˜ê²½ ë³€ìˆ˜(API í‚¤ ë“±)ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
from dotenv import load_dotenv

# JSON ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json

# LangChain ê´€ë ¨ í•µì‹¬ ëª¨ë“ˆë“¤
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# --- ë…¼ìˆ  ì²¨ì‚­ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ ---

class EssayGrader:
    """
    ë…¼ìˆ  ì‹œí—˜ ìë£Œë¥¼ ë¡œë“œí•˜ì—¬ ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê³ ,
    RAG ì²´ì¸ì„ í†µí•´ í•™ìƒ ë‹µì•ˆì„ ì²¨ì‚­í•˜ëŠ” í´ë˜ìŠ¤.
    """

    # í´ë˜ìŠ¤ê°€ ìƒì„±ë  ë•Œ ë‹¨ í•œ ë²ˆ ì‹¤í–‰ë˜ëŠ” ì´ˆê¸°í™” í•¨ìˆ˜
    def __init__(self, json_path: str):
        """
        í´ë˜ìŠ¤ ì´ˆê¸°í™” ì‹œ í•„ìš”í•œ ëª¨ë“  ì¤€ë¹„ ì‘ì—…ì„ ìˆ˜í–‰.
        1. API í‚¤ ë¡œë“œ
        2. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
        3. ë…¼ìˆ  ìë£Œ(JSON) ë¡œë“œ ë° ê°€ê³µ
        4. FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        5. LLM ë° RAG ì²´ì¸ ì„¤ì •

        Args:
            json_path (str): ë²¡í„° DBë¥¼ êµ¬ì¶•í•  ë…¼ìˆ  ìë£Œ JSON íŒŒì¼ ê²½ë¡œ.
        """
        print("ë…¼ìˆ  ì²¨ì‚­ê¸° ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # 1. OpenAI API í‚¤ ì„¤ì •
        self._setup_api_key()

        # 2. 'ì˜ë¯¸ ë²ˆì—­ê¸°'(ì„ë² ë”© ëª¨ë¸) ì¤€ë¹„
        self.embedding_model = self._initialize_embedding_model()

        # 3. JSON ë°ì´í„° ë¡œë“œ ë° Document ê°ì²´ë¡œ ë³€í™˜
        structured_docs = self._load_and_structure_data(json_path)

        # 4. 'ì´ˆê³ ì† ë””ì§€í„¸ ë„ì„œê´€'(ë²¡í„° DB) êµ¬ì¶•
        print("\në¬¸ì„œ ì¡°ê°ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        self.vector_db = FAISS.from_documents(structured_docs, self.embedding_model)
        self.retriever = self.vector_db.as_retriever()
        print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

        # 5. RAG ì²´ì¸ ì¡°ë¦½
        self.correction_chain = self._build_rag_chain()
        print("âœ… AI ë…¼ìˆ  ì²¨ì‚­ RAG ì²´ì¸ ì™„ì„±!")
        print("\n--- ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì²¨ì‚­ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")

    # ë¹„ê³µê°œ í—¬í¼ í•¨ìˆ˜: API í‚¤ ì„¤ì •
    def _setup_api_key(self):
        """ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ë¡œë“œí•˜ì—¬ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì • """
        load_dotenv()
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("í™˜ê²½ë³€ìˆ˜ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("âœ… API í‚¤ ë¡œë”© ì™„ë£Œ.")

    # ë¹„ê³µê°œ í—¬í¼ í•¨ìˆ˜: ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    def _initialize_embedding_model(self):
        """ HuggingFace ì„ë² ë”© ëª¨ë¸(ko-sbert-nli)ì„ ë¡œë“œ """
        print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)")
        model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-nli",
            model_kwargs={'device': 'cpu'}, # GPUê°€ ìˆë‹¤ë©´ 'cuda'ë¡œ ë³€ê²½ ê°€ëŠ¥
            encode_kwargs={'normalize_embeddings': True},
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
        return model

    # ë¹„ê³µê°œ í—¬í¼ í•¨ìˆ˜: ë°ì´í„° ë¡œë“œ ë° êµ¬ì¡°í™”
    def _load_and_structure_data(self, json_path: str):
        """ JSON íŒŒì¼ì„ ì½ì–´ LangChainì˜ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ """
        print(f"ë…¼ìˆ  ìë£Œ '{json_path}' íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤.")
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"'{json_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        structured_docs = []
        base_metadata = {
            "university": data.get("university", "ì •ë³´ ì—†ìŒ"),
            "year": data.get("year", "ì •ë³´ ì—†ìŒ"),
            "subject": data.get("subject", "ì •ë³´ ì—†ìŒ")
        }
        content_map = {
            "ì¶œì œì˜ë„": data.get("intended_purpose"),
            "ì±„ì ê¸°ì¤€": data.get("grading_criteria"),
            "ëª¨ë²”ë‹µì•ˆ": data.get("sample_answer")
        }

        for content_type, content in content_map.items():
            if content:
                doc = Document(
                    page_content=content,
                    metadata={**base_metadata, "content_type": content_type, "question_id": data.get("question_id")}
                )
                structured_docs.append(doc)
        
        print(f"âœ… ì´ {len(structured_docs)}ê°œì˜ ë…¼ë¦¬ì  ë¬¸ì„œ ì¡°ê° ìƒì„± ì™„ë£Œ.")
        return structured_docs

    # ë¹„ê³µê°œ í—¬í¼ í•¨ìˆ˜: RAG ì²´ì¸ êµ¬ì¶•
    def _build_rag_chain(self):
        """ ë…¼ìˆ  ì²¨ì‚­ì„ ìœ„í•œ RAG ì²´ì¸ì„ êµ¬ì„± """
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7) # gpt-4.1-nano ëŒ€ì²´
        output_parser = StrOutputParser()
        
        prompt_template = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ ëŒ€ì¹˜ë™ì—ì„œ 10ë…„ê°„ ë…¼ìˆ ì„ ê°€ë¥´ì¹œ, ëƒ‰ì² í•˜ì§€ë§Œ ì• ì • ì–´ë¦° ì¡°ì–¸ì„ ì•„ë¼ì§€ ì•ŠëŠ” ìŠ¤íƒ€ê°•ì‚¬ 'ë…¼ë¦¬ì™• ê¹€ë©˜í† 'ì…ë‹ˆë‹¤. í•™ìƒì˜ ëˆˆë†’ì´ì— ë§ì¶° í•µì‹¬ì„ ê¿°ëš«ëŠ” 'íŒ©íŠ¸ í­ê²©'ê³¼ ë”°ëœ»í•œ ê²©ë ¤ë¥¼ ê²¸ë¹„í•œ ì²¨ì‚­ ìŠ¤íƒ€ì¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.

        [ì…ë ¥ ì •ë³´]
        1. [ì±„ì  ê¸°ì¤€]: {retrieved_scoring_criteria}
        2. [ëª¨ë²” ë‹µì•ˆ]: {retrieved_model_answer}
        3. [í•™ìƒ ë‹µì•ˆ]: {user_ocr_answer}

        [ì²¨ì‚­ ì ˆì°¨ ë° ì§€ì‹œ]
        ë‹¹ì‹ ì€ ì•„ë˜ 4ë‹¨ê³„ì˜ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³, ìµœì¢… ì²¨ì‚­ë¬¸ì„ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        1. (ì´í•´): ë¨¼ì €, [í•™ìƒ ë‹µì•ˆ]ì„ í•œ ë¬¸ë‹¨ì”© ì½ìœ¼ë©° ì „ì²´ì ì¸ ë…¼ë¦¬ì˜ íë¦„ê³¼ í•µì‹¬ ì£¼ì¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
        2. (ë¹„êµ): ê·¸ ë‹¤ìŒ, í•™ìƒ ë‹µì•ˆì˜ ê° ë¬¸ë‹¨ì´ [ì±„ì  ê¸°ì¤€]ì˜ ì–´ë–¤ í•­ëª©ì— ë¶€í•©í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  [ëª¨ë²” ë‹µì•ˆ]ì˜ ë…¼ë¦¬ êµ¬ì¡°ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
        3. (í‰ê°€): ë¶„ì„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê° í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ì¹­ì°¬ê³¼ ê°œì„ ì ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
        4. (ì¢…í•©): ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìµœì¢… ì²¨ì‚­ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤.

        [ì¶œë ¥ í˜•ì‹]
        ---
        **[ì´í‰]**
        (í•™ìƒ ë‹µì•ˆì˜ ì „ë°˜ì ì¸ ê°•ì ê³¼ ì•½ì ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ë‚ ì¹´ë¡­ê²Œ ìš”ì•½)

        **[ì˜í•œ ì  (ì¹­ì°¬ í¬ì¸íŠ¸) ğŸ‘]**
        - (ì±„ì  ê¸°ì¤€ê³¼ ë¹„êµí•˜ì—¬, í•™ìƒ ë‹µì•ˆì´ ì–´ë–¤ ì ì—ì„œ í›Œë¥­í•œì§€ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ë¬¸ì¥ì„ ì¸ìš©í•˜ì—¬ ì¹­ì°¬)

        **[ì•„ì‰¬ìš´ ì  (ê°œì„  í¬ì¸íŠ¸) âœï¸]**
        - (ëª¨ë²”ë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬, ì–´ë–¤ ë¶€ë¶„ì„ ë³´ì™„í•˜ë©´ ë” ì¢‹ì€ ê¸€ì´ ë  ìˆ˜ ìˆì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ)

        **[ì´ë ‡ê²Œ ë°”ê¿”ë³´ì„¸ìš” (ëŒ€ì•ˆ ë¬¸ì¥ ì œì•ˆ) ğŸ’¡]**
        - (ì•„ì‰¬ìš´ ì ìœ¼ë¡œ ì§€ì ëœ ë¬¸ì¥ 1~2ê°œë¥¼, ë” ë…¼ë¦¬ì ì´ê³  ì„¸ë ¨ëœ ë¬¸ì¥ìœ¼ë¡œ ì§ì ‘ ìˆ˜ì •í•´ì„œ ì œì•ˆí•©ë‹ˆë‹¤. ë°˜ë“œì‹œ "í•™ìƒ ì›ë¬¸: ..." -> "ìˆ˜ì • ì œì•ˆ: ..." í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì—¬ëŸ¬ ê°œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.)

        **[ì˜ˆìƒ ì ìˆ˜ ë° ë‹¤ìŒ í•™ìŠµ íŒ ğŸš€]**
        - (ì±„ì  ê¸°ì¤€ì„ ê·¼ê±°ë¡œ ì˜ˆìƒ ì ìˆ˜ë¥¼ ì œì‹œí•˜ê³ , ì´ í•™ìƒì´ ë‹¤ìŒë²ˆì— ë” ì„±ì¥í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ í•™ìŠµ íŒì„ 1ê°€ì§€ ì œì•ˆ)
        ---
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)

        # RAG ì²´ì¸ ì¡°ë¦½
        chain = (
            {
                "retrieved_model_answer": RunnableLambda(lambda x: self.retriever.invoke(f"{x['question_info']} ëª¨ë²”ë‹µì•ˆ")),
                "retrieved_scoring_criteria": RunnableLambda(lambda x: self.retriever.invoke(f"{x['question_info']} ì±„ì ê¸°ì¤€")),
                "user_ocr_answer": lambda x: x["user_ocr_answer"]
            }
            | prompt
            | llm
            | output_parser
        )
        return chain

    # ê³µê°œ ë©”ì†Œë“œ: í•™ìƒ ë‹µì•ˆ ì²¨ì‚­ ì‹¤í–‰
    def grade_essay(self, question_info: str, student_answer: str) -> str:
        """
        í•™ìƒì˜ ë‹µì•ˆì„ ë°›ì•„ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê³  ì²¨ì‚­ ê²°ê³¼ë¥¼ ë°˜í™˜.

        Args:
            question_info (str): ë¬¸ì œ ì •ë³´ (ì˜ˆ: "2023ë…„ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµ ì¸ë¬¸ë…¼ìˆ  ë¬¸ì œ 2ë²ˆ")
            student_answer (str): í•™ìƒì´ ì‘ì„±í•œ ë‹µì•ˆ í…ìŠ¤íŠ¸.

        Returns:
            str: LLMì´ ìƒì„±í•œ ì²¨ì‚­ ê²°ê³¼ ë¬¸ìì—´.
        """
        print(f"'{question_info}'ì— ëŒ€í•œ ì²¨ì‚­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        return self.correction_chain.invoke({
            "question_info": question_info,
            "user_ocr_answer": student_answer
        })

    # ê³µê°œ ë©”ì†Œë“œ: ëª¨ë²” ë‹µì•ˆ ì›ë¬¸ ê²€ìƒ‰
    def get_model_answer(self, question_info: str) -> str:
        """
        ë²¡í„° DBì—ì„œ í•´ë‹¹ ë¬¸ì œì˜ ëª¨ë²” ë‹µì•ˆ ì›ë¬¸ì„ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜.

        Args:
            question_info (str): ë¬¸ì œ ì •ë³´.

        Returns:
            str: ê²€ìƒ‰ëœ ëª¨ë²” ë‹µì•ˆ í…ìŠ¤íŠ¸. ì°¾ì§€ ëª»í•˜ë©´ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜.
        """
        model_answer_docs = self.retriever.invoke(f"{question_info} ëª¨ë²”ë‹µì•ˆ")
        return model_answer_docs[0].page_content if model_answer_docs else "ëª¨ë²”ë‹µì•ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."