# essay_grader.py

# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import os
from dotenv import load_dotenv
import json

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# --- ë…¼ìˆ  ì²¨ì‚­ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ ---
class EssayGrader:
    """
    ë…¼ìˆ  ì‹œí—˜ ìë£Œë¥¼ ë¡œë“œí•˜ì—¬ ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê³ ,
    RAG ì²´ì¸ì„ í†µí•´ í•™ìƒ ë‹µì•ˆì„ ì²¨ì‚­í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, json_path: str):
        print("ë…¼ìˆ  ì²¨ì‚­ê¸° ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self._setup_api_key()
        self.embedding_model = self._initialize_embedding_model()
        structured_docs = self._load_and_structure_data(json_path)
        print("\në¬¸ì„œ ì¡°ê°ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        self.vector_db = FAISS.from_documents(structured_docs, self.embedding_model)
        self.retriever = self.vector_db.as_retriever()
        print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        self.correction_chain = self._build_rag_chain()
        print("âœ… AI ë…¼ìˆ  ì²¨ì‚­ RAG ì²´ì¸ ì™„ì„±!")
        print("\n--- ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì²¨ì‚­ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")

    def _setup_api_key(self):
        load_dotenv()
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("í™˜ê²½ë³€ìˆ˜ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("âœ… API í‚¤ ë¡œë”© ì™„ë£Œ.")

    def _initialize_embedding_model(self):
        print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)")
        model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-nli",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
        return model

    def _load_and_structure_data(self, json_path: str):
        print(f"ë…¼ìˆ  ìë£Œ '{json_path}' íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤.")
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"'{json_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        structured_docs = []
        base_metadata = {
            "university": data.get("university", "ì •ë³´ ì—†ìŒ"),
            "year": data.get("year", "ì •ë³´ ì—†ìŒ"),
            "subject": data.get("subject", "ì •ë³´ ì—†ìŒ")
        }
        content_map = {
            "ì¶œì œì˜ë„": data.get("intended_purpose"),
            "ì±„ì ê¸°ì¤€": data.get("grading_criteria"),
            "ëª¨ë²”ë‹µì•ˆ": data.get("sample_answer")
        }
        for content_type, content in content_map.items():
            if content:
                doc = Document(
                    page_content=content,
                    metadata={**base_metadata, "content_type": content_type, "question_id": data.get("question_id")}
                )
                structured_docs.append(doc)
        print(f"âœ… ì´ {len(structured_docs)}ê°œì˜ ë…¼ë¦¬ì  ë¬¸ì„œ ì¡°ê° ìƒì„± ì™„ë£Œ.")
        return structured_docs

    def _build_rag_chain(self):
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
        output_parser = StrOutputParser()
        prompt_template = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ ëŒ€ì¹˜ë™ì—ì„œ 10ë…„ê°„ ë…¼ìˆ ì„ ê°€ë¥´ì¹œ, ëƒ‰ì² í•˜ì§€ë§Œ ì• ì • ì–´ë¦° ì¡°ì–¸ì„ ì•„ë¼ì§€ ì•ŠëŠ” ìŠ¤íƒ€ê°•ì‚¬ 'ë…¼ë¦¬ì™• ê¹€ë©˜í† 'ì…ë‹ˆë‹¤. í•™ìƒì˜ ëˆˆë†’ì´ì— ë§ì¶° í•µì‹¬ì„ ê¿°ëš«ëŠ” 'íŒ©íŠ¸ í­ê²©'ê³¼ ë”°ëœ»í•œ ê²©ë ¤ë¥¼ ê²¸ë¹„í•œ ì²¨ì‚­ ìŠ¤íƒ€ì¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.
        [ì…ë ¥ ì •ë³´]
        1. [ì±„ì  ê¸°ì¤€]: {retrieved_scoring_criteria}
        2. [ëª¨ë²” ë‹µì•ˆ]: {retrieved_model_answer}
        3. [í•™ìƒ ë‹µì•ˆ]: {user_ocr_answer}
        [ì²¨ì‚­ ì ˆì°¨ ë° ì§€ì‹œ]
        ë‹¹ì‹ ì€ ì•„ë˜ 4ë‹¨ê³„ì˜ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³, ìµœì¢… ì²¨ì‚­ë¬¸ì„ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        1. (ì´í•´): ë¨¼ì €, [í•™ìƒ ë‹µì•ˆ]ì„ í•œ ë¬¸ë‹¨ì”© ì½ìœ¼ë©° ì „ì²´ì ì¸ ë…¼ë¦¬ì˜ íë¦„ê³¼ í•µì‹¬ ì£¼ì¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
        2. (ë¹„êµ): ê·¸ ë‹¤ìŒ, í•™ìƒ ë‹µì•ˆì˜ ê° ë¬¸ë‹¨ì´ [ì±„ì  ê¸°ì¤€]ì˜ ì–´ë–¤ í•­ëª©ì— ë¶€í•©í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  [ëª¨ë²” ë‹µì•ˆ]ì˜ ë…¼ë¦¬ êµ¬ì¡°ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
        3. (í‰ê°€): ë¶„ì„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê° í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ì¹­ì°¬ê³¼ ê°œì„ ì ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
        4. (ì¢…í•©): ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìµœì¢… ì²¨ì‚­ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤.
        [ì¶œë ¥ í˜•ì‹]
        ---
        **[ì´í‰]**
        (í•™ìƒ ë‹µì•ˆì˜ ì „ë°˜ì ì¸ ê°•ì ê³¼ ì•½ì ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ë‚ ì¹´ë¡­ê²Œ ìš”ì•½)
        **[ì˜í•œ ì  (ì¹­ì°¬ í¬ì¸íŠ¸) ğŸ‘]**
        - (ì±„ì  ê¸°ì¤€ê³¼ ë¹„êµí•˜ì—¬, í•™ìƒ ë‹µì•ˆì´ ì–´ë–¤ ì ì—ì„œ í›Œë¥­í•œì§€ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ë¬¸ì¥ì„ ì¸ìš©í•˜ì—¬ ì¹­ì°¬)
        **[ì•„ì‰¬ìš´ ì  (ê°œì„  í¬ì¸íŠ¸) âœï¸]**
        - (ëª¨ë²”ë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬, ì–´ë–¤ ë¶€ë¶„ì„ ë³´ì™„í•˜ë©´ ë” ì¢‹ì€ ê¸€ì´ ë  ìˆ˜ ìˆì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ)
        **[ì´ë ‡ê²Œ ë°”ê¿”ë³´ì„¸ìš” (ëŒ€ì•ˆ ë¬¸ì¥ ì œì•ˆ) ğŸ’¡]**
        - (ì•„ì‰¬ìš´ ì ìœ¼ë¡œ ì§€ì ëœ ë¬¸ì¥ 1~2ê°œë¥¼, ë” ë…¼ë¦¬ì ì´ê³  ì„¸ë ¨ëœ ë¬¸ì¥ìœ¼ë¡œ ì§ì ‘ ìˆ˜ì •í•´ì„œ ì œì•ˆí•©ë‹ˆë‹¤. ë°˜ë“œì‹œ "í•™ìƒ ì›ë¬¸: ..." -> "ìˆ˜ì • ì œì•ˆ: ..." í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì—¬ëŸ¬ ê°œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.)
        **[ì˜ˆìƒ ì ìˆ˜ ë° ë‹¤ìŒ í•™ìŠµ íŒ ğŸš€]**
        - (ì±„ì  ê¸°ì¤€ì„ ê·¼ê±°ë¡œ ì˜ˆìƒ ì ìˆ˜ë¥¼ ì œì‹œí•˜ê³ , ì´ í•™ìƒì´ ë‹¤ìŒë²ˆì— ë” ì„±ì¥í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ í•™ìŠµ íŒì„ 1ê°€ì§€ ì œì•ˆ)
        ---
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = (
            {
                "retrieved_model_answer": RunnableLambda(lambda x: self.retriever.invoke(f"{x['question_info']} ëª¨ë²”ë‹µì•ˆ")),
                "retrieved_scoring_criteria": RunnableLambda(lambda x: self.retriever.invoke(f"{x['question_info']} ì±„ì ê¸°ì¤€")),
                "user_ocr_answer": lambda x: x["user_ocr_answer"]
            }
            | prompt
            | llm
            | output_parser
        )
        return chain

    def grade_essay(self, question_info: str, student_answer: str) -> str:
        print(f"'{question_info}'ì— ëŒ€í•œ ì²¨ì‚­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        return self.correction_chain.invoke({
            "question_info": question_info,
            "user_ocr_answer": student_answer
        })

    def get_model_answer(self, question_info: str) -> str:
        model_answer_docs = self.retriever.invoke(f"{question_info} ëª¨ë²”ë‹µì•ˆ")
        return model_answer_docs[0].page_content if model_answer_docs else "ëª¨ë²”ë‹µì•ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."