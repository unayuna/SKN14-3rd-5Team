# essay_grader.py (ë‹¨ìˆœí™”ëœ ìµœì¢… ë²„ì „)

import os
from dotenv import load_dotenv
import pickle
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

FAISS_INDEX_DIR = './01_data_preprocessing/faiss'
DOCUMENT_CACHE_PATH = "./01_data_preprocessing/faiss/preprocessed_documents.pkl"

def safe_retriever_invoke(retriever, query, source_type):
    docs = retriever.get_relevant_documents(query)
    # if docs:
    #     return "\n".join([doc.page_content for doc in docs])
    for doc in docs:
        if doc.metadata.get("source_type") == source_type:
            return doc.page_content
    return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

class EssayGrader:
    def __init__(self):
        print("ë…¼ìˆ  ì²¨ì‚­ê¸° ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self._setup_api_key()
        self.embedding_model = self._initialize_embedding_model()
        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
        
        if os.path.exists(FAISS_INDEX_DIR):
            print(f"\nğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ '{FAISS_INDEX_DIR}'ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
            self.vector_db = FAISS.load_local(FAISS_INDEX_DIR, self.embedding_model, allow_dangerous_deserialization=True)
        else:
            print(f"\nğŸ“„ ì „ì²˜ë¦¬ëœ pickle íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
            with open(DOCUMENT_CACHE_PATH, 'rb') as f:
                all_documents = pickle.load(f)
            print(f"âœ… ì´ {len(all_documents)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ë¡œë”© ì™„ë£Œ!")
            
            print("ğŸ“Œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            self.vector_db = FAISS.from_documents(all_documents, self.embedding_model)
            self.vector_db.save_local(FAISS_INDEX_DIR)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ë¥¼ '{FAISS_INDEX_DIR}'ì— ì €ì¥ ì™„ë£Œ!")

        self.retriever = self.vector_db.as_retriever()
        print("âœ… ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ!")

        self.correction_chain = self._build_rag_chain()
        print("âœ… AI ë…¼ìˆ  ì²¨ì‚­ RAG ì²´ì¸ ì™„ì„±!")
        print("\n--- ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì²¨ì‚­ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")
    
    def _setup_api_key(self):
        load_dotenv()
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("í™˜ê²½ë³€ìˆ˜ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("âœ… API í‚¤ ë¡œë”© ì™„ë£Œ.")

    def _initialize_embedding_model(self):
        print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)")
        model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-nli",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
        return model

    def _build_rag_chain(self):
        output_parser = StrOutputParser()
        prompt_template = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ ëŒ€ì¹˜ë™ì—ì„œ 10ë…„ê°„ ë…¼ìˆ ì„ ê°€ë¥´ì¹œ, ëƒ‰ì² í•˜ì§€ë§Œ ì• ì • ì–´ë¦° ì¡°ì–¸ì„ ì•„ë¼ì§€ ì•ŠëŠ” ìŠ¤íƒ€ê°•ì‚¬ 'ë…¼ë¦¬ì™• ê¹€ë©˜í† 'ì…ë‹ˆë‹¤. í•™ìƒì˜ ëˆˆë†’ì´ì— ë§ì¶° í•µì‹¬ì„ ê¿°ëš«ëŠ” 'íŒ©íŠ¸ í­ê²©'ê³¼ ë”°ëœ»í•œ ê²©ë ¤ë¥¼ ê²¸ë¹„í•œ ì²¨ì‚­ ìŠ¤íƒ€ì¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.

        [ì…ë ¥ ì •ë³´]
        1. [ì±„ì  ê¸°ì¤€]: {retrieved_scoring_criteria}
        2. [ëª¨ë²” ë‹µì•ˆ]: {retrieved_model_answer}
        3. [í•™ìƒ ë‹µì•ˆ]: {user_ocr_answer}

        [ì²¨ì‚­ ì ˆì°¨ ë° ì§€ì‹œ]
        ë‹¹ì‹ ì€ ì•„ë˜ 4ë‹¨ê³„ì˜ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³, ìµœì¢… ì²¨ì‚­ë¬¸ì„ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        1. (ì´í•´): ë¨¼ì €, [í•™ìƒ ë‹µì•ˆ]ì„ í•œ ë¬¸ë‹¨ì”© ì½ìœ¼ë©° ì „ì²´ì ì¸ ë…¼ë¦¬ì˜ íë¦„ê³¼ í•µì‹¬ ì£¼ì¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
        2. (ë¹„êµ): ê·¸ ë‹¤ìŒ, í•™ìƒ ë‹µì•ˆì˜ ê° ë¬¸ë‹¨ì´ [ì±„ì  ê¸°ì¤€]ì˜ ì–´ë–¤ í•­ëª©ì— ë¶€í•©í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  [ëª¨ë²” ë‹µì•ˆ]ì˜ ë…¼ë¦¬ êµ¬ì¡°ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë•Œ, ê° ëŒ€í•™ë³„ë¡œ ì±„ì  ê¸°ì¤€ì„ ë©´ë°€íˆ ì‚´í´ë³´ê³  ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•„ì„œ ì¡°ì–¸ì— ë°˜ì˜í•©ë‹ˆë‹¤.
        3. (í‰ê°€): ë¶„ì„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê° í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ì¹­ì°¬ê³¼ ê°œì„ ì ì„ ì •ë¦¬í•©ë‹ˆë‹¤.â€ƒ
        4. (ì¢…í•©): ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ìµœì¢… ì²¨ì‚­ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤.

        [ì¶œë ¥ í˜•ì‹]
        ---
        **[ì´í‰]**
        (í•™ìƒ ë‹µì•ˆì˜ ì „ë°˜ì ì¸ ê°•ì ê³¼ ì•½ì ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ë‚ ì¹´ë¡­ê²Œ ìš”ì•½)

        **[ì˜í•œ ì  (ì¹­ì°¬ í¬ì¸íŠ¸) ğŸ‘]**
        - (ì±„ì  ê¸°ì¤€ê³¼ ë¹„êµí•˜ì—¬, í•™ìƒ ë‹µì•ˆì´ ì–´ë–¤ ì ì—ì„œ í›Œë¥­í•œì§€ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ë¬¸ì¥ì„ ì¸ìš©í•˜ì—¬ ì¹­ì°¬)

        **[ì•„ì‰¬ìš´ ì  (ê°œì„  í¬ì¸íŠ¸) âœï¸]**
        - (ëª¨ë²”ë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬, ì–´ë–¤ ë¶€ë¶„ì„ ë³´ì™„í•˜ë©´ ë” ì¢‹ì€ ê¸€ì´ ë  ìˆ˜ ìˆì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ)

        **[ì´ë ‡ê²Œ ë°”ê¿”ë³´ì„¸ìš” (ëŒ€ì•ˆ ë¬¸ì¥ ì œì•ˆ) ğŸ’¡]**
        - **ì•„ë˜ ì§€ì‹œë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”: **[ì•„ì‰¬ìš´ ì ]ì—ì„œ ì§€ì í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê°œì„ í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ì„ ìµœì†Œ 3ê°œ ê³¨ë¼** ë” ë…¼ë¦¬ì ì´ê³  ì„¸ë ¨ëœ ë¬¸ì¥ìœ¼ë¡œ ì§ì ‘ ìˆ˜ì •í•´ì„œ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤.**
        - **ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ "í•™ìƒ ì›ë¬¸: (í•™ìƒì˜ ì›ë˜ ë¬¸ì¥)" ë‹¤ìŒ ì¤„ì— "ìˆ˜ì • ì œì•ˆ: (AIê°€ ìˆ˜ì •í•œ ë¬¸ì¥)" í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.**
        - (ì˜ˆì‹œ)
        í•™ìƒ ì›ë¬¸: "í†µì¼ì‹ ë¼ëŠ” ìƒˆë¡œìš´ ì •ì²´ì„±ì„ ë§Œë“¤ì–´ì„œ ì„±ê³µí–ˆê³ , ì˜êµ­ì€ ì˜›ë‚  ì •ì²´ì„±ì— ë¨¸ë¬¼ëŸ¬ì„œ ì‹¤íŒ¨í•œ ê²ƒ ê°™ë‹¤."
        ìˆ˜ì • ì œì•ˆ: "í†µì¼ì‹ ë¼ëŠ” 'ì‚¼í•œì¼í†µì˜ì‹'ì´ë¼ëŠ” í†µí•©ì  ì •ì²´ì„±ì„ ìƒˆë¡­ê²Œ ì •ë¦½í•˜ì—¬ êµ­ê°€ì  ë°œì „ì„ ì´ë£©í•œ ë°˜ë©´, ì˜êµ­ì€ ê¸°ì¡´ì˜ ì •ì²´ì„±ì—ë§Œ ë¨¸ë¬¼ëŸ¬ ë¸Œë ‰ì‹œíŠ¸ë¼ëŠ” ì •ì±…ì  í•œê³„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤."

        **[ì˜ˆìƒ ì ìˆ˜ ë° ë‹¤ìŒ í•™ìŠµ íŒ ğŸš€]**
        - (ì±„ì  ê¸°ì¤€ì„ ê·¼ê±°ë¡œ ì˜ˆìƒ ì ìˆ˜ë¥¼ ì œì‹œí•˜ê³ , ì´ í•™ìƒì´ ë‹¤ìŒë²ˆì— ë” ì„±ì¥í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ í•™ìŠµ íŒì„ 1ê°€ì§€ ì œì•ˆ)
        ---
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = (
            {
                "retrieved_model_answer": RunnableLambda(lambda x: safe_retriever_invoke(self.retriever, x["question_id"], "ëª¨ë²”ë‹µì•ˆ")),
                "retrieved_scoring_criteria": RunnableLambda(lambda x: safe_retriever_invoke(self.retriever, x["question_id"], "ì±„ì ê¸°ì¤€")),
                "user_ocr_answer": lambda x: x["user_ocr_answer"],
                "question_id": lambda x: x["question_id"]
            }
            | prompt
            | self.llm
            | output_parser
        )
        return chain

    def grade_essay(self, question_id: str, student_answer: str) -> str:
        print(f"'{question_id}'ì— ëŒ€í•œ ì²¨ì‚­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        return self.correction_chain.invoke({
            "question_id": question_id,
            "user_ocr_answer": student_answer
        })

    def get_document_content(self, question_id: str, source_type: str) -> str:
        for doc in self.vector_db.docstore._dict.values():
            if doc.metadata.get("question_id") == question_id and doc.metadata.get("source_type") == source_type:
                print("### ìš”ì²­í•œ ì¿¼ë¦¬")
                print(f"{question_id}")
                print("### ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ (from Retriever)")
                print(f"`{doc.metadata}`")
                return doc.page_content
        return f"{source_type}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # Documents ê²€ìƒ‰ ì¶œë ¥ìš©
    # def get_document_content(self, question_id: str, source_type: str) -> str:
    #     import streamlit as st  # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

    #     for doc in self.vector_db.docstore._dict.values():
    #         if doc.metadata.get("question_id") == question_id and doc.metadata.get("source_type") == source_type:
    #             st.markdown("### ğŸ“Œ ìš”ì²­í•œ ì¿¼ë¦¬")
    #             st.write(f"ë¬¸í•­ ID: `{question_id}`, ìš”ì²­ ìœ í˜•: `{source_type}`")

    #             st.markdown("### ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ (from VectorDB)")
    #             st.write(f"`{doc.metadata}`")
    #             st.code(doc.page_content[:500])  # ì•ë¶€ë¶„ë§Œ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ

    #             return doc.page_content

    #     return f"{source_type}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def mento_chat(self, grading_criteria: str, sample_answer: str, user_answer: str, followup_question: str, history=[]) -> str:
        prompt = f"""
    [ì—­í• ]
    ë‹¹ì‹ ì€ ëŒ€ì¹˜ë™ì—ì„œ 10ë…„ê°„ ë…¼ìˆ ì„ ê°€ë¥´ì¹œ, ëƒ‰ì² í•˜ì§€ë§Œ ì• ì • ì–´ë¦° ì¡°ì–¸ì„ ì•„ë¼ì§€ ì•ŠëŠ” ìŠ¤íƒ€ê°•ì‚¬ 'ë…¼ë¦¬ì™• ê¹€ë©˜í† 'ì…ë‹ˆë‹¤.

    [ì…ë ¥ ì •ë³´]
    1. [ì±„ì  ê¸°ì¤€]: {grading_criteria}
    2. [ëª¨ë²” ë‹µì•ˆ]: {sample_answer}
    3. [í•™ìƒ ë‹µì•ˆ]: {user_answer}

    [ì²¨ì‚­ ì ˆì°¨ ë° ì§€ì‹œ]
    1. (ì´í•´) í•™ìƒ ë‹µì•ˆì„ ì „ì²´ì ìœ¼ë¡œ ì½ê³  í•µì‹¬ ì£¼ì¥ íŒŒì•…
    2. (ë¹„êµ) ì±„ì  ê¸°ì¤€ ë° ì˜ˆì‹œë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬ ë¶„ì„
    3. (í‰ê°€) ì¥ë‹¨ì  ëª…ì‹œ
    4. (ì¢…í•©) ì²¨ì‚­ ë¬¸ì¥ ì™„ì„±

    [ì¶œë ¥ í˜•ì‹]
    ---
    **[ì´í‰]**
    ...

    **[ì˜í•œ ì  (ì¹­ì°¬ í¬ì¸íŠ¸) ğŸ‘]**
    ...

    **[ì•„ì‰¬ìš´ ì  (ê°œì„  í¬ì¸íŠ¸) âœï¸]**
    ...

    **[ì´ë ‡ê²Œ ë°”ê¿”ë³´ì„¸ìš” (ëŒ€ì•ˆ ë¬¸ì¥ ì œì•ˆ) ğŸ’¡]**
    ...

    **[ì˜ˆìƒ ì ìˆ˜ ë° ë‹¤ìŒ í•™ìŠµ íŒ ğŸš€]**
    ...

    [ì¶”ê°€ ì§ˆë¬¸]
    {followup_question}
    """
        messages = [{"role": "system", "content": "ë„ˆëŠ” ë…¼ë¦¬ì™• ê¹€ë©˜í† ë¡œ í–‰ë™í•´. ìœ„ ì •ë³´ì— ë”°ë¼ í•™ìƒì—ê²Œ ë…¼ë¦¬ì ì´ê³  ì• ì • ì–´ë¦° í”¼ë“œë°±ì„ ì œê³µí•´."}]
        messages.append({"role": "user", "content": prompt})
        for h in history:
            messages.append({"role": "user", "content": h["user"]})
            messages.append({"role": "assistant", "content": h["assistant"]})

        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
        return llm.invoke(messages).content.strip()

