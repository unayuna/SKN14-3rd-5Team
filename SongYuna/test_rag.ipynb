{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpeOCUh0y/NarvD/Xpbfys"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RAG & Prompt"],"metadata":{"id":"DJb6ekUOMnYr"}},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n","os.environ[\"HF_TOKNE\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"8R_Zt42sN679","executionInfo":{"status":"ok","timestamp":1752569952173,"user_tz":-540,"elapsed":4109,"user":{"displayName":"유나","userId":"05684396509889141976"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 로드 및 청크"],"metadata":{"id":"wvXSVqWgOWZA"}},{"cell_type":"code","source":["import json\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","json_filename = 'ajou_2023_1.json'\n","json_dir = './data'\n","json_file_path = os.path.join(json_dir, json_filename)\n","# json 파일 로드\n","with open('ajou_2023_1.json', 'r', encoding='utf-8') as f:\n","  data = json.load(f)\n","\n","print('json 필드:', data.keys())\n","print('question_id:', data['question_id'])\n","\n","# Text Splitter 초기화 (청크 작업)\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200,\n","    length_function=len\n",")\n","\n","# metadata 확인 및 추가\n","initial_docs = []\n","base_metadata = {\n","    \"university\": \"아주대학교\",\n","    \"year\": 2023,\n","    \"subject\": \"인문논술\"\n","}\n","\n","# content_type과 해당 내용을 매핑\n","content_map = {\n","    \"출제의도\": data.get(\"intended_purpose\"),\n","    \"채점기준\": data.get(\"grading_criteria\"),\n","    \"모범답안\": data.get(\"sample_answer\")\n","}\n","\n","for content_type, content in content_map.items():\n","    if content:\n","        # content를 청크로 분할, 각 청크는 원본 doc의 메타데이터를 상속\n","        metadata={\n","            **base_metadata,\n","            \"content_type\": content_type,\n","            \"source_file\": json_filename\n","        }\n","        # chunk\n","        chunks = text_splitter.create_documents(\n","            texts=[content],\n","            metadatas=[metadata]\n","        )\n","        docs.append(chunks)\n","\n","print(len(docs))\n","docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m2VQMaQOQju","executionInfo":{"status":"ok","timestamp":1752570467345,"user_tz":-540,"elapsed":52,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"913ce0f4-8cff-4aae-899a-7876f4a0f4d6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["json 필드: dict_keys(['question_id', 'intended_purpose', 'grading_criteria', 'sample_answer'])\n","question_id: 2023_아주대_인문_1\n","3\n"]},{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'university': '아주대학교', 'year': 2023, 'subject': '인문논술', 'content_type': '출제의도', 'question_id': '2023_아주대_인문_1'}, page_content='\\n')"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## 임베딩 및 벡터db 저장"],"metadata":{"id":"CbdoJ0pCUxa5"}},{"cell_type":"code","source":["# 문서 임베딩 및 벡터db 저장\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","import torch\n","\n","# embedding = OpenAIEmbeddings 사용\n","embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n","\n","# vector_db = FAISS\n","vector_db = FAISS.from_documents(\n","    documents=docs,\n","    embedding=embeddings\n",")\n","vector_db.save_local('./db/faiss')"],"metadata":{"id":"FfCsMk8gU0i7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# test"],"metadata":{"id":"pEKddkJ4oRZV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aHMayCFHx3S"},"outputs":[],"source":["pip install langchain langchain-openai faiss-cpu langchain_core python-dotenv"]},{"cell_type":"markdown","source":["## 파일 경로 설정 및 모델 설정\n","- 파일 경로(json, pdf, faiss_db)\n","- Text Splitter\n","- Embedding"],"metadata":{"id":"5g4j-RPzoPfm"}},{"cell_type":"code","source":["import json\n","import os\n","\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","# --- 1. 설정 변수 정의 ---\n","JSON_DATA_DIR = './data_json'  # 논술 문제 JSON 파일들이 저장된 디렉토리 경로\n","PDF_FILES_DIR = './test_pdf' # 논술 문제 PDF 파일들이 저장된 디렉토리 (벡터 DB에 직접 포함X, 메타데이터로만 연결)\n","BASE_FAISS_DB_DIR = './faiss_test_db' # 각 문제별 FAISS 인덱스(DB)를 저장할 최상위 디렉토리\n","\n","# --- 2. 텍스트 분할기(Text Splitter) 초기화 ---\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1500,\n","    chunk_overlap=200,\n","    separators=['\\n\\n', '\\n', '.', ' ', ''],\n","    length_function=len,\n","    is_separator_regex=False,\n",")\n","\n","# --- 3. 임베딩 모델 설정 ---\n","embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"],"metadata":{"id":"TWNS-sqHXVdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## FAISS 문제별 인덱스 생성 및 Document & Chunk 설정\n","- json 디렉토리를 내 파일을 순회하면서 인덱스를 생성한다 *filename = FAISS 인덱스명\n","- 각 인덱스마다 Document를 생성한다\n","- Document -> chunk 한후\n","- 벡터db에 from_document 방식으로 저장한다"],"metadata":{"id":"t8svFcDSjPxj"}},{"cell_type":"code","source":["# --- 4. 모든 JSON 파일을 순회하며 각 문제별 FAISS 인덱스 생성 ---\n","for filename in os.listdir(JSON_DATA_DIR):\n","    if filename.endswith('.json'):\n","        json_file_path = os.path.join(JSON_DATA_DIR, filename) # 현재 JSON 파일의 전체 경로\n","\n","        with open(json_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f) # JSON 데이터를 파이썬 딕셔너리로 변환!! json.load()\n","\n","        # metadata 추가하기\n","        question_id = data[\"question_id\"] # 무조건 존재 -> FAISS 인덱스명\n","        university = data.get(\"university\", \"Unknown University\")\n","        year = data.get(\"year\", 0)\n","        subject = data.get(\"subject\", \"Unknown Subject\")\n","\n","        # pdf *파일명이 json과 동일할 것\n","        pdf_filename = f\"{os.path.splitext(filename)[0]}.pdf\"\n","        pdf_path = os.path.join(PDF_FILES_DIR, pdf_filename)\n","\n","        docs = []\n","\n","        base_metadata = {\n","            \"university\": university,\n","            \"year\": year,\n","            \"subject\": subject,\n","            \"question_id\": question_id,\n","            \"json_path\": filename, # 이 청크의 원본 JSON 파일명\n","            \"pdf_path\": pdf_path, # 이 청크와 관련된 PDF 파일의 경로\n","        }\n","\n","        content = {\n","            \"출제의도\": data.get(\"intended_purpose\"),\n","            \"채점기준\": data.get(\"grading_criteria\"),\n","            \"모범답안\": data.get(\"sample_answer\")\n","        }\n","\n","        for content_type, content_text in content.items():\n","            if content_text:\n","                doc = Document(\n","                    page_content=content_text,\n","                    metadata={\n","                        **base_metadata,\n","                        \"content_type\": content_type,\n","                    }\n","                )\n","                docs.append(doc)\n","\n","        # Document -> chunk\n","        chunks = splitter.split_documents(docs)\n","\n","\n","        # 문제별 FAISS 인덱스 생성 및 저장\n","        if chunks:\n","            test_faiss_path = os.path.join(BASE_FAISS_DB_DIR, question_id) # (예: './faiss_test_db/{question_id}')\n","            os.makedirs(test_faiss_path, exist_ok=True) # 없다면 생성\n","\n","            vector_db = FAISS.from_documents(\n","                documents=chunks, # Document 후 자른 chunk를 전달!\n","                embedding=embeddings\n","            )\n","            # FAISS DB를 로컬 파일로 저장: index.faiss(벡터데이터)와 index.pkl(메타데이터)\n","            vector_db.save_local(test_faiss_path)\n","            print(f\" '{question_id}' ({len(chunks)} 청크) 생성 완료: {test_faiss_path}\")\n","\n","\n","print(f\"\\n모든 문제별 FAISS 인덱스 생성이 완료되었습니다. 저장 경로: '{BASE_FAISS_DB_DIR}'\")"],"metadata":{"id":"eos7P8XXYVPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## FAISS 인덱스 로드 및 직접 사용\n","- FAISS.load_local"],"metadata":{"id":"permjcebjLwe"}},{"cell_type":"code","source":["selected_question_id = \"2024_아주대_인문_1\"\n","\n","selected_faiss_path = os.path.join(BASE_FAISS_DB_DIR, selected_question_id)\n","\n","if os.path.exists(selected_faiss_path):\n","    loaded_vector_db = FAISS.load_local(selected_faiss_path, embeddings, allow_dangerous_deserialization=True)\n","    print(f\"'{selected_question_id}' 인덱스가 성공적으로 로드되었습니다.\")\n","\n","    query = \"이 문제의 채점 기준에 대해 자세히 알려줘.\"\n","\n","    retrieved_docs = vector_db.similarity_search(\n","        query,\n","        k=3,\n","        filter={\"content_type\": \"채점기준\"}\n","    )\n","\n","    print(f\"\\n쿼리: '{query}'에 대한 검색 결과:\")\n","    for i, doc in enumerate(retrieved_docs):\n","        print(f\"메타데이터: {doc.metadata}\")\n","        print(f\"내용 미리보기: {doc.page_content[:200]}...\")"],"metadata":{"id":"uXjEnOfjiJGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retriever 및 Chain 생성"],"metadata":{"id":"ON8NFUdar-09"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.prompts import PromptTemplate\n","\n","llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0)\n","retriever = vector_db.as_retriever({\"k\": 3, \"filter\": {\"content_type\": \"채점기준\"} })\n","\n","prompt = PromptTemplate.from_template(\n","    \"다음은 논술문제 관련 질의입니다. 질문: {query}\"\n",")\n","\n","selected_question_id = \"2024_아주대_인문_1\" # {question_id} 의 content\n","selected_faiss_path = os.path.join(BASE_FAISS_DB_DIR, selected_question_id)\n","vector_db = FAISS.load_local(\n","    documents=selected_faiss_path,\n","    embedding=embeddings,\n","    allow_dangerous_deserialization=True # allow_dangerous_deserialization\n","    )\n","\n","\n","rag_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever,\n","    chain_type_kwargs={\"prompt\": prompt}\n",")\n","query = \"이 문제의 채점 기준에 대해 자세히 알려줘.\"\n","response = rag_chain.invoke({\"query\": query})\n","\n","print(f\"답변: {response['result']}\")"],"metadata":{"id":"sGLZ8ST1sCKj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"7Dl9OqAyJ_wF"}},{"cell_type":"markdown","source":["## requests"],"metadata":{"id":"WBoegpsFKB3H"}},{"cell_type":"code","source":["%pip install langchain langchain-openai faiss-cpu langchain_core python-dotenv langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7h7UWL3qKBmO","executionInfo":{"status":"ok","timestamp":1752642957740,"user_tz":-540,"elapsed":11796,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"ca1184c1-a2dc-4f42-ce8e-804b5866d92f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.28)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n","Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.68)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.94.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.1)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.9)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n","Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"markdown","source":["## 환경설정 import"],"metadata":{"id":"vfaQc0aIKFiG"}},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1752642962970,"user_tz":-540,"elapsed":489,"user":{"displayName":"유나","userId":"05684396509889141976"}},"id":"Bie4o1WqKN7E"},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import json\n","from dotenv import load_dotenv\n","\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n","\n","load_dotenv()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JCC_oFCjKIrX","executionInfo":{"status":"ok","timestamp":1752642965077,"user_tz":-540,"elapsed":375,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"b5aa6d58-1e68-42a4-a167-b8600781c74f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## 설정 및 변수 선언"],"metadata":{"id":"QLbzsq41Kix1"}},{"cell_type":"code","source":["JSON_DIR = './data_json'\n","PDF_DIR = './test_pdf'\n","VECTOR_DB_DIR = './test_vector_db'\n","\n","# text splitter 준비\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1500,\n","    chunk_overlap=200,\n","    separators=['\\n\\n', '\\n', '.', ' ', ''],\n","    length_function=len,\n","    is_separator_regex=False,\n",")\n","\n","# 임베딩 모델 및 LLM\n","embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n","llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.01)\n","\n","print(\"설정 변수 정의 및 초기화 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzJAWQCyKiSM","executionInfo":{"status":"ok","timestamp":1752643644945,"user_tz":-540,"elapsed":201,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"d8c38822-936f-4949-b355-1bc87ccbd8bc"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["설정 변수 정의 및 초기화 완료!\n"]}]},{"cell_type":"markdown","source":["## FAISS 인덱스 생성 및 저장"],"metadata":{"id":"ZEGO1dO1ND9-"}},{"cell_type":"code","source":["chunks = []\n","\n","# 디렉토리 생성 (없으면 만들기)\n","os.makedirs(JSON_DIR, exist_ok=True)\n","os.makedirs(PDF_DIR, exist_ok=True)\n","os.makedirs(VECTOR_DB_DIR, exist_ok=True)\n","\n","for filename in os.listdir(JSON_DIR):\n","    if filename.endswith('.json'):\n","        json_file_path = os.path.join(JSON_DIR, filename)\n","\n","        with open(json_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f) # JSON -> 파이썬(딕셔너리)\n","\n","        # 메타데이터 생성\n","        print(os.path.splitext(filename)[0])\n","        question_id = data.get(\"question_id\", os.path.splitext(filename)[0])\n","        # university = data.get(\"university\", \"Unknown University\")\n","        # year = data.get(\"year\", 0)\n","        # subject = data.get(\"subject\", \"Unknown Subject\")\n","\n","        content_map = {\n","            \"출제의도\": data.get(\"intended_purpose\"),\n","            \"채점기준\": data.get(\"grading_criteria\"),\n","            \"모범답안\": data.get(\"sample_answer\")\n","        }\n","\n","        current_docs = []\n","        for content_type, content_text in content_map.items():\n","            if content_text:\n","                doc = Document(\n","                    page_content=content_text,\n","                    metadata={\n","                        # \"university\": university,\n","                        # \"year\": year,\n","                        # \"subject\": subject,\n","                        \"question_id\": question_id,\n","                        \"content_type\": content_type,\n","                    }\n","                )\n","                current_docs.append(doc)\n","\n","        # 현재 파일의 document(current_docs)를 chunk(current_chunk) 함 -> chunks의 저장\n","        current_chunk = splitter.split_documents(current_docs)\n","        chunks.extend(current_chunk) # extend 사용!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t92qkFnKNJ-S","executionInfo":{"status":"ok","timestamp":1752643650000,"user_tz":-540,"elapsed":59,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"f6b92e9c-418d-4809-d597-87136421f2c4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["ajou_2023_1\n","ajou_2024_1\n"]}]},{"cell_type":"code","source":["# FAISS 인덱스 생성\n","if chunks:\n","    vector_db = FAISS.from_documents(\n","        documents=chunks, # 모든 청크를 document로!!\n","        embedding=embeddings\n","    )\n","    # FAISS DB를 로컬 파일 저장\n","    vector_db.save_local(VECTOR_DB_DIR)\n","    print(f\"\\n모든 문제 데이터({len(chunks)} 청크)를 포함하는 단일 FAISS 인덱스 생성 완료: '{VECTOR_DB_DIR}'\")\n","else:\n","    print(\"\\n경고: 처리할 JSON 데이터가 없습니다. FAISS 인덱스를 생성하지 않았습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDAPKx26QlZY","executionInfo":{"status":"ok","timestamp":1752643657847,"user_tz":-540,"elapsed":1760,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"dffa7746-e0da-4a5a-9548-012316aa3042"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","모든 문제 데이터(0 청크)를 포함하는 단일 FAISS 인덱스 생성 완료: './test_vector_db'\n"]}]},{"cell_type":"markdown","source":["## FAISS 인덱스 로드 및 Retriever 생성"],"metadata":{"id":"PKYCGWjPQuEG"}},{"cell_type":"code","source":["# 저장된 통합 FAISS DB를 메모리로 불러옵니다.\n","loaded_vector_db = FAISS.load_local(\n","    VECTOR_DB_DIR,\n","    embeddings,\n","    allow_dangerous_deserialization=True\n",")\n","print(f\"FAISS 인덱스가 성공적으로 로드되었습니다. 경로: '{VECTOR_DB_DIR}'\")\n","\n","# Retriever 생성: 모든 문서에서 검색\n","retriever = loaded_vector_db.as_retriever(search_kwargs={\"k\": 5})\n","print(\"Retriever 생성 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoiO6MstQthI","executionInfo":{"status":"ok","timestamp":1752643720062,"user_tz":-540,"elapsed":55,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"f8d0ce96-0879-4634-d2c1-9c238767f49e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["FAISS 인덱스가 성공적으로 로드되었습니다. 경로: './test_vector_db'\n","Retriever 생성 완료!\n"]}]},{"cell_type":"markdown","source":["## Chain 생성\n","- 논술 답안 첨삭 체인\n","- 논술 답안 첨삭 관련 질문 챗봇 체인"],"metadata":{"id":"6jKhnBa_Rnue"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def get_doc_text(query: str) -> str:\n","    docs = retriever.invoke(query)\n","    return \"\\n\".join(doc.page_content for doc in docs)\n","\n","\n","# 1. 프롬프트 템플릿\n","essay_prompt = PromptTemplate.from_template(\"\"\"\n","당신은 대학입시 논술전형 채점관입니다. 다음 정보를 바탕으로 학생의 답안을 엄격하게 평가하고, 상세한 피드백을 제공해주세요.\n","\n","---\n","**논술 문제 정보:**\n","- 출제 의도: {intended_purpose}\n","- 채점 기준: {grading_criteria}\n","- 모범 답안: {sample_answer}\n","\n","---\n","**학생의 답안:**\n","{user_essay}\n","\n","---\n","**평가 가이드라인:**\n","(이하 생략...)\n","\"\"\")\n","\n","# 2. RAG 기반 체인 구성\n","essay_chain = (\n","    RunnableParallel(\n","        {\n","            # 사용자가 입력한 에세이\n","            \"user_essay\": lambda x: x[\"user_essay\"],\n","\n","            # 검색 기반 필드 - 질문 정보 기반 쿼리\n","            \"intended_purpose\": RunnableLambda(lambda x: get_doc_text(f\"{x['question_info']} 출제의도\")),\n","            \"grading_criteria\": RunnableLambda(lambda x: get_doc_text(f\"{x['question_info']} 채점기준\")),\n","            \"sample_answer\": RunnableLambda(lambda x: get_doc_text(f\"{x['question_info']} 모범답안\")),\n","        }\n","    )\n","    | essay_prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","print(\"논술 첨삭 체인 생성 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ji1jSAWRpr1","executionInfo":{"status":"ok","timestamp":1752643726080,"user_tz":-540,"elapsed":23,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"e046495b-96e5-41a6-cbe9-0897cd8f9ab3"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["논술 첨삭 체인 생성 완료!\n"]}]},{"cell_type":"code","source":["# 논술 관련 질의응답 챗봇 체인\n","essay_qa_prompt = PromptTemplate.from_template(\"\"\"\n","주어진 문맥 정보를 바탕으로 질문에 답변해주세요.\n","만약 문맥 정보에 답변이 없다면, \"주어진 정보만으로는 답변하기 어렵습니다.\"라고 답해주세요.\n","추가적인 정보는 생성하지 마세요.\n","\n","문맥:\n","{context}\n","\n","질문: {query}\n","답변:\"\"\"\n",")\n","\n","# Retriever -> 프롬프트 -> LLM -> 결과 파싱\n","essay_qa_chain = (\n","    RunnableParallel(\n","        {\"context\": retriever | (lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n","         \"question\": RunnablePassthrough()}\n","    )\n","    | essay_qa_prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","print(\"논술 관련 질의응답 챗봇 체인 생성완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lHpbR_iT7Ue","executionInfo":{"status":"ok","timestamp":1752643732018,"user_tz":-540,"elapsed":13,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"4cc56a2e-5ffd-44dd-9a91-72d00fcc9b09"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["논술 관련 질의응답 챗봇 체인 생성완료!\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"sTKLylQVWan6"}},{"cell_type":"code","source":["output = essay_chain.invoke({\n","    \"question_info\": \"2023 한국외대 인문논술 1번\",\n","    \"user_essay\": \"다른 학부모가 배신하면 자신도 배신한다.\"\n","})\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDngFoixoBl3","executionInfo":{"status":"ok","timestamp":1752643818847,"user_tz":-540,"elapsed":24345,"user":{"displayName":"유나","userId":"05684396509889141976"}},"outputId":"b8078eb2-92ae-4d8d-94a8-aebccded36db"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["학생의 답안을 평가하기 위해 주어진 정보를 바탕으로 분석해보겠습니다.\n","\n","**학생의 답안 분석:**\n","학생의 답안은 \"다른 학부모가 배신하면 자신도 배신한다.\"라는 한 문장으로 구성되어 있습니다. 이 문장은 주어진 문제의 요구사항을 충분히 반영하지 못하고 있습니다. 문제 2-2는 (다)와 (라)의 학부모의 행동을 비교하고, 그 결과를 예측하는 것을 요구하고 있습니다. 학생의 답안은 (라)의 학부모의 행동을 부분적으로 언급하고 있지만, (다)의 학부모의 행동이나 두 학부모의 행동을 비교하는 내용이 전혀 포함되어 있지 않습니다.\n","\n","**채점 기준에 따른 평가:**\n","1. **(다)의 학부모의 행동 예측 (4점):** 학생의 답안에는 (다)의 학부모에 대한 언급이 전혀 없습니다. 따라서 이 부분에 대한 점수는 0점입니다.\n","   \n","2. **(라)의 학부모의 행동 예측 (4점):** 학생의 답안은 (라)의 학부모가 \"다른 학부모가 배신하면 자신도 배신한다\"는 점을 언급하고 있습니다. 이 부분은 부분적으로 맞지만, (라)의 학부모가 협력할 경우에 대한 언급이 없으므로 완전한 답변이 아닙니다. 따라서 이 부분에 대한 점수는 2점입니다.\n","\n","3. **(다)와 (라)의 학부모의 선호 비교 (3점):** 학생의 답안은 (다)와 (라)의 학부모의 선호를 비교하지 않았습니다. 따라서 이 부분에 대한 점수는 0점입니다.\n","\n","4. **(다)의 학부모의 선호 설명 (3점):** (다)의 학부모에 대한 설명이 없으므로 이 부분에 대한 점수는 0점입니다.\n","\n","5. **(라)의 학부모의 선호 설명 (3점):** 학생의 답안은 (라)의 학부모의 배신에 대한 선호를 언급했지만, 협력에 대한 선호는 언급하지 않았습니다. 따라서 이 부분에 대한 점수는 1점입니다.\n","\n","**총점: 3/17**\n","\n","**피드백:**\n","학생의 답안은 문제의 요구사항을 충분히 반영하지 못했습니다. (다)와 (라)의 학부모의 행동을 모두 설명하고, 그들의 행동을 비교하는 것이 필요합니다. 또한, 각 학부모의 행동에 대한 예측과 그 이유를 명확히 설명해야 합니다. 다음 번에는 문제의 요구사항을 꼼꼼히 읽고, 각 부분을 충실히 답변하도록 노력하세요.\n"]}]}]}