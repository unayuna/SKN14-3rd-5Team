# data_preprocessor.py (ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê°•í™” ë²„ì „)

import os
import json
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


JSON_DIR = "data_json"
PDF_DIR = 'test_pdf'
OUTPUT_FILE = "processed_data.pkl"


# 1. text splitter ì¤€ë¹„
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
    separators=['\n\n', '\n', '.', ' ', ''],
    length_function=len,
    is_separator_regex=False,
)

# 2. ì„ë² ë”© ëª¨ë¸ ë° LLM
# embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
# llm = ChatOpenAI(model_name="gpt-4o", temperature=0.01)
# print("ì„¤ì • ë³€ìˆ˜ ì •ì˜ ë° ì´ˆê¸°í™” ì™„ë£Œ!")

chunks = []

def process_json_data():

    print(f"--- '{JSON_DIR}' í´ë”ì˜ JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ---")
    if not os.path.exists(JSON_DIR):
        print(f"[ì˜¤ë¥˜] '{JSON_DIR}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    for filename in os.listdir(JSON_DIR):
        if filename.endswith(".json"):
            filepath = os.path.join(JSON_DIR, filename)

            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # 3. ë©”íƒ€ë°ì´í„° ìƒì„±
            print(os.path.splitext(filename)[0])
            question_id = data.get("question_id", os.path.splitext(filename)[0])

            if not question_id:
                print(f"[ê²½ê³ ] {filename}ì— 'question_id'ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                question_id = filename.replace(".json", "")

            # # [í•µì‹¬ ìˆ˜ì •] ë©”íƒ€ë°ì´í„°ëŠ” íŒŒì¼ëª…ì´ ì•„ë‹Œ, question_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
            # # ì˜ˆ: "2023_ì„œê°•ëŒ€_1" -> parts = ['2023', 'ì„œê°•ëŒ€', '1']
            # parts = filename.replace(".json", "").split('_')
            
            # # ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œ, ì˜ˆì™¸ ìƒí™©ì— ëŒ€í•œ ë°©ì–´ ì½”ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            # # íŒŒì¼ëª… êµ¬ì¡°ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëŒ€ì²˜
            # university = parts[0] if len(parts) > 0 else "ì•Œìˆ˜ì—†ìŒ"
            # year = parts[1] if len(parts) > 1 else "ì•Œìˆ˜ì—†ìŒ"
            # number = parts[2] if len(parts) > 2 else "ê¸°íƒ€"

            # base_metadata = {
            #     "question_id": question_id,
            #     "university": university,
            #     "year": year,
            #     "number": number
            # }

            content_map = {
                "ì¶œì œì˜ë„": data.get("intended_purpose"),
                "ì±„ì ê¸°ì¤€": data.get("grading_criteria"),
                "ëª¨ë²”ë‹µì•ˆ": data.get("sample_answer")
            }
            
            current_docs = []
            for content_type, content_text in content_map.items():
                if content_text:
                    doc = Document(
                        page_content=content_text,
                        metadata={
                            "question_id": question_id,
                            "content_type": content_type,
                        }
                    )
                    current_docs.append(doc)
                print(f"{filename} ì²˜ë¦¬ ì™„ë£Œ. (ID: {question_id})")
            
            # 4. chunk
            current_chunk = splitter.split_documents(current_docs)
            chunks.extend(current_chunk) # extend ì‚¬ìš©!

    if not current_docs:
        print("[ê²½ê³ ] ì²˜ë¦¬í•  ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
        return None


#     with open(OUTPUT_FILE, 'wb') as f:
#         pickle.dump(current_docs, f)
#     print(f"\nğŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(current_docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê°ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     return current_docs

# if __name__ == '__main__':
#     processed_data = process_json_data()
#     if processed_data:
#         print("\n[ìƒ˜í”Œ ë°ì´í„° í™•ì¸]")
#         for doc in processed_data[:5]:
#             print(doc, "\n" + "-"*30)