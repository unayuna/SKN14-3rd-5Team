import os
import json
from dotenv import load_dotenv

from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate

load_dotenv(dotenv_path="../essay_test/.env")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")
DATA_DIR = "../essay_test/data"



def load_data():
    all_data = {}
    for fname in os.listdir(DATA_DIR):
        if fname.endswith(".json"):
            with open(os.path.join(DATA_DIR, fname), "r", encoding="utf-8") as f:
                data = json.load(f)
            qid = data["question_id"]
            year, school, qnum = qid.split("_", 2)
            all_data[qid] = {
                "year": year,
                "school": school,
                "qnum": qnum,
                "data": data,
                "file": fname
            }
    return all_data



def build_vectorstore(question_bank):
    docs = []
    for qid, item in question_bank.items():
        content = f"ì§ˆë¬¸ ID: {qid}\n"
        content += f"[ì¶œì œ ëª©ì ]\n{item['data'].get('intended_purpose')}\n\n"
        content += f"[ì±„ì  ê¸°ì¤€]\n{item['data'].get('grading_criteria')}\n\n"
        content += f"[ì˜ˆì‹œ ë‹µì•ˆ]\n{item['data'].get('sample_answer')}"
        doc = Document(page_content=content, metadata={"question_id": qid})
        docs.append(doc)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=OPENAI_EMBEDDING_MODEL)
    return FAISS.from_documents(splits, embeddings)


def generate_feedback(llm, question_id, grading_criteria, sample_answer, user_answer, vectorstore):
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3, "filter": {"question_id": question_id}}
    )
    docs = retriever.get_relevant_documents("")
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"""
[ì—­í• ]
ë‹¹ì‹ ì€ ëŒ€ì¹˜ë™ì—ì„œ 10ë…„ê°„ ë…¼ìˆ ì„ ê°€ë¥´ì¹œ, ëƒ‰ì² í•˜ì§€ë§Œ ì• ì • ì–´ë¦° ì¡°ì–¸ì„ ì•„ë¼ì§€ ì•ŠëŠ” ìŠ¤íƒ€ê°•ì‚¬ 'ë…¼ë¦¬ì™• ê¹€ë©˜í† 'ìž…ë‹ˆë‹¤.

[ìž…ë ¥ ì •ë³´]
1. [ì±„ì  ê¸°ì¤€]: {grading_criteria}
2. [ëª¨ë²” ë‹µì•ˆ]: {sample_answer}
3. [í•™ìƒ ë‹µì•ˆ]: {user_answer}

[ì²¨ì‚­ ì ˆì°¨ ë° ì§€ì‹œ]
1. (ì´í•´) í•™ìƒ ë‹µì•ˆì„ ì „ì²´ì ìœ¼ë¡œ ì½ê³  í•µì‹¬ ì£¼ìž¥ íŒŒì•…
2. (ë¹„êµ) ì±„ì  ê¸°ì¤€ ë° ì˜ˆì‹œë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬ ë¶„ì„
3. (í‰ê°€) ìž¥ë‹¨ì  ëª…ì‹œ
4. (ì¢…í•©) ì²¨ì‚­ ë¬¸ìž¥ ì™„ì„±

[ì¶œë ¥ í˜•ì‹]
---
**[ì´í‰]**
...

**[ìž˜í•œ ì  (ì¹­ì°¬ í¬ì¸íŠ¸) ðŸ‘]**
...

**[ì•„ì‰¬ìš´ ì  (ê°œì„  í¬ì¸íŠ¸) âœï¸]**
...

**[ì´ë ‡ê²Œ ë°”ê¿”ë³´ì„¸ìš” (ëŒ€ì•ˆ ë¬¸ìž¥ ì œì•ˆ) ðŸ’¡]**
...

**[ì˜ˆìƒ ì ìˆ˜ ë° ë‹¤ìŒ í•™ìŠµ íŒ ðŸš€]**
...
"""
    result = llm.invoke(prompt)
    return result.content

# class AnswerChatRAG:
#     def __init__(self, user_answer, openai_api_key, embedding_model="text-embedding-ada-002"):
#         self.user_answer = user_answer
#         self.openai_api_key = openai_api_key
#         self.embedding_model = embedding_model
#         self.build_vectorstore = self.build_vectorstore()
#         self.retriever = self.build_vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
#         self.build_chain = self.build_chain()
#
#     # ìœ ì € ë‹µë³€ -> ë²¡í„°ìŠ¤í† ì–´ ë³€í™˜
#     def build_vectorstore(self):
#         doc = Document(page_content=self.user_answer)
#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
#         splits = text_splitter.split_documents([doc])
#         embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key, model=self.embedding_model)
#         return FAISS.from_documents(splits, embeddings)
#
#     # ë²¡í„°ìŠ¤í† ì–´ + í”„ë¡¬í”„íŠ¸ + LLM -> ì²´ì¸ êµ¬ì„±
#     def build_chain(self,vectorstore, openai_api_key):
#         retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
#
#         # ì²´ì¸ êµ¬ì„±
#         prompt = PromptTemplate.from_template("""
#                     ë‹¹ì‹ ì€ 10ë…„ ì´ìƒ ìˆ˜ëŠ¥ ë° ëŒ€í•™ ë…¼ìˆ ì„ ì „ë¬¸ì ìœ¼ë¡œ ê°€ë¥´ì³ì˜¨ ì²¨ì‚­ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
#                     í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•™ìƒì´ ìž‘ì„±í•œ ë…¼ìˆ  ë¬¸ìž¥ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
#
#                     [ì œì‹œ ë¬¸ìž¥]
#                     ì•„ëž˜ëŠ” ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ì„ íƒëœ í•™ìƒì˜ ë‹µì•ˆ ë‚´ìš© ì¼ë¶€ìž…ë‹ˆë‹¤. ì°¸ê³ í•´ ë¶„ì„ì— í™œìš©í•˜ì„¸ìš”.
#
#                     {context}
#
#                     [í•™ìƒ ì§ˆë¬¸]
#                     {question}
#
#                     [ë‹µë³€ ì§€ì¹¨]
#                     1. ì§ˆë¬¸ì˜ ìš”ì§€ë¥¼ íŒŒì•…í•˜ê³ , ë‹µì•ˆ ë¬¸ìž¥ ì¤‘ ê´€ë ¨ ìžˆëŠ” ë‚´ìš©ì„ ì—°ê²°í•´ í•´ì„í•©ë‹ˆë‹¤.
#                     2. ë¶€ì¡±í•˜ê±°ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìžˆë‹¤ë©´ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  êµ¬ì²´ì ì¸ ë¬¸ìž¥ ë˜ëŠ” ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.
#                     3. í”¼ë“œë°±ì€ ì¹œì ˆí•˜ê³  ì¡°ë¦¬ ìžˆê²Œ ì œì‹œí•˜ë˜, ë…¼ë¦¬ì„±ê³¼ êµ¬ì¡°ì  ì‚¬ê³ ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìžˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
#                     4. í•™ìƒì´ ìž˜ ì´í•´í•  ìˆ˜ ìžˆë„ë¡ ê¸¸ê³  êµ¬ì²´ì ìœ¼ë¡œ, ìƒì„¸ížˆ ë‹µë³€í•´ì¤ë‹ˆë‹¤.
#
#                     [ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ]
#                     ### ðŸ§  ë¶„ì„
#                     - (ì§ˆë¬¸ ìš”ì§€ë¥¼ ìš”ì•½í•˜ê³ , í•™ìƒ ë‹µì•ˆì—ì„œ ê´€ë ¨ ë¬¸ìž¥ì„ ì–´ë–»ê²Œ í•´ì„í–ˆëŠ”ì§€ ì„¤ëª…)
#
#                     ### ðŸ’¡ ê°œì„  ì œì•ˆ
#                     - (ë³´ë‹¤ ë‚˜ì€ ë¬¸ìž¥ í‘œí˜„ / ë…¼ë¦¬ ì „ê°œ / ì‚¬ë¡€ ì¶”ê°€ ë“± êµ¬ì²´ì  ê°œì„  ë°©ë²• ì œì•ˆ)
#
#                     ### ðŸ—’ï¸ ì˜ˆì‹œ ë‹µë³€
#                     - (ë¶„ì„ê³¼ ê°œì„  ì œì•ˆì„ í† ëŒ€ë¡œ ëª¨ë²” ë‹µì•ˆ í˜¹ì€ ì§„í–‰ ë°©í–¥ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ê¸°)
#
#                     ### ðŸ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
#                     - (ì¢…í•© ì •ë¦¬ì™€ í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ëŒ€ë¹„ í•™ìŠµ íŒ)
#
#                     [ë‹µë³€]
#                     """)
#
#         chain = (
#                 {
#                     "context": lambda x: "\n\n".join([
#                         doc.page_content for doc in retriever.get_relevant_documents(x["question"])
#                     ]),
#                     "question_id": lambda x: x["question_id"]
#                 }
#                 | prompt
#                 | ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)
#                 | StrOutputParser()
#         )
#         return chain
#
#     def invoke(self, question_id):
#         return self.build_chain.invoke({"question_id": question_id})


class AnswerChatRAG:
    def __init__(self, user_answer, openai_api_key, embedding_model="text-embedding-ada-002"):
        self.user_answer = user_answer
        self.openai_api_key = openai_api_key
        self.embedding_model = embedding_model

        self.vectorstore = self.build_vectorstore()
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": 3}
        )
        self.chain = self.build_chain()

    # ë‹µë³€ì„ ë²¡í„°ìŠ¤í† ì–´ë¡œ ì „í™˜
    def build_vectorstore(self):
        doc = Document(page_content=self.user_answer)
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = splitter.split_documents([doc])
        embeddings = OpenAIEmbeddings(
            openai_api_key=self.openai_api_key, model=self.embedding_model
        )
        return FAISS.from_documents(splits, embeddings)

    # ì²´ì¸ êµ¬ì„± (í”„ë¡¬í”„íŠ¸ + LLM + ì¶œë ¥ íŒŒì„œ)
    def build_chain(self):
        prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ 10ë…„ ì´ìƒ ìˆ˜ëŠ¥ ë° ëŒ€í•™ ë…¼ìˆ ì„ ì „ë¬¸ì ìœ¼ë¡œ ê°€ë¥´ì³ì˜¨ ì²¨ì‚­ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•™ìƒì´ ìž‘ì„±í•œ ë…¼ìˆ  ë¬¸ìž¥ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

[ì œì‹œ ë¬¸ìž¥]
ì•„ëž˜ëŠ” ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ì„ íƒëœ í•™ìƒì˜ ë‹µì•ˆ ë‚´ìš© ì¼ë¶€ìž…ë‹ˆë‹¤. ì°¸ê³ í•´ ë¶„ì„ì— í™œìš©í•˜ì„¸ìš”.

{context}

[í•™ìƒ ì§ˆë¬¸]
{question}

[ë‹µë³€ ì§€ì¹¨]
1. ì§ˆë¬¸ì˜ ìš”ì§€ë¥¼ íŒŒì•…í•˜ê³ , ë‹µì•ˆ ë¬¸ìž¥ ì¤‘ ê´€ë ¨ ìžˆëŠ” ë‚´ìš©ì„ ì—°ê²°í•´ í•´ì„í•©ë‹ˆë‹¤.
2. ë¶€ì¡±í•˜ê±°ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìžˆë‹¤ë©´ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  êµ¬ì²´ì ì¸ ë¬¸ìž¥ ë˜ëŠ” ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.
3. í”¼ë“œë°±ì€ ì¹œì ˆí•˜ê³  ì¡°ë¦¬ ìžˆê²Œ ì œì‹œí•˜ë˜, ë…¼ë¦¬ì„±ê³¼ êµ¬ì¡°ì  ì‚¬ê³ ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìžˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
4. í•™ìƒì´ ìž˜ ì´í•´í•  ìˆ˜ ìžˆë„ë¡ ê¸¸ê³  êµ¬ì²´ì ìœ¼ë¡œ, ìƒì„¸ížˆ ë‹µë³€í•´ì¤ë‹ˆë‹¤.

[ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ]
### ðŸ§  ë¶„ì„
- (ì§ˆë¬¸ ìš”ì§€ë¥¼ ìš”ì•½í•˜ê³ , í•™ìƒ ë‹µì•ˆì—ì„œ ê´€ë ¨ ë¬¸ìž¥ì„ ì–´ë–»ê²Œ í•´ì„í–ˆëŠ”ì§€ ì„¤ëª…)

### ðŸ’¡ ê°œì„  ì œì•ˆ
- (ë³´ë‹¤ ë‚˜ì€ ë¬¸ìž¥ í‘œí˜„ / ë…¼ë¦¬ ì „ê°œ / ì‚¬ë¡€ ì¶”ê°€ ë“± êµ¬ì²´ì  ê°œì„  ë°©ë²• ì œì•ˆ)

### ðŸ—’ï¸ ì˜ˆì‹œ ë‹µë³€
- (ë¶„ì„ê³¼ ê°œì„  ì œì•ˆì„ í† ëŒ€ë¡œ ëª¨ë²” ë‹µì•ˆ í˜¹ì€ ì§„í–‰ ë°©í–¥ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ê¸°)

### ðŸ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
- (ì¢…í•© ì •ë¦¬ì™€ í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ëŒ€ë¹„ í•™ìŠµ íŒ)

[ë‹µë³€]
""")
        chain = (
            {
                "context": lambda x: "\n\n".join([
                    doc.page_content for doc in self.retriever.get_relevant_documents(x["question"])
                ]),
                "question": lambda x: x["question"]
            }
            | prompt
            | ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=self.openai_api_key)
            | StrOutputParser()
        )
        return chain

    # ì™¸ë¶€ì—ì„œ ì²´ì¸ í˜¸ì¶œ
    def invoke(self, question: str) -> str:
        return self.chain.invoke({"question": question})